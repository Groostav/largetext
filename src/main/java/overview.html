<!DOCTYPE html>
<!--
  ~ Copyright (c) 2014, Francis Galiegue (fgaliegue@gmail.com)
  ~
  ~ This software is dual-licensed under:
  ~
  ~ - the Lesser General Public License (LGPL) version 3.0 or, at your option, any
  ~   later version;
  ~ - the Apache Software License (ASL) version 2.0.
  ~
  ~ The text of both licenses is available under the src/resources/ directory of
  ~ this project (under the names LGPL-3.0.txt and ASL-2.0.txt respectively).
  ~
  ~ Direct link to the sources:
  ~
  ~ - LGPL 3.0: https://www.gnu.org/licenses/lgpl-3.0.txt
  ~ - ASL 2.0: http://www.apache.org/licenses/LICENSE-2.0.txt
  -->

<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>largetext: use regexes on large text files</title>
</head>
<body>
<h1>largetext: use large text files as CharSequences</h1>

<h1>Why</h1>

<p>This project started life as a proof of concept; it stemmed from a <a
href="http://stackoverflow.com/q/22017480/1093528" target="_blank">discussion
on StackOverflow</a>, where the user wanted to search for a pattern (using a
{@link java.util.regex.Matcher} on a large text file.</p>

<p>The suggested and accepted answer (mine) was to implement {@link
java.lang.CharSequence} over such a large text file, since a {@link
java.util.regex.Matcher} expects a <code>CharSequence</code> as an argument.
This interface is implemented, among other things, by {@link java.lang.String}
and {@link java.nio.CharBuffer}.</p>

<p>And now, {@link com.github.fge.largetext.LargeText} as well.</p>

<p>Note that this can not only be used with regexes: you can also use this
project with <a href="https://github.com/parboiled1/grappa">Grappa</a>. Yes, you
can use arbitrary grammars entirely in Java on large text files!</p>

<h1>The challenge(s)</h1>

<p>Implementing this did not go without a number of problems to solve; and most
of these problems come from the {@link java.lang.CharSequence} interface
itself. And to start with:</p>

<h2>Querying operations might sleep</h2>

<p>And <code>CharSequence</code> does not account for this; in fact, it pretty
much assumes that all the characters you deal with are readily accessible, which
is not the case here.</p>

<p>As a consequence, you cannot throw {@link java.lang.InterruptedException}...
It is therefore the responsibility of the implementation to deal correctly with
that. Here, it is dealt with by terminating the relevant threads and throwing an
unchecked exception: {@link com.github.fge.largetext.LargeTextException}.</p>

<h2>Accesses may be concurrent, and anywhere in the sequence</h2>

<p>While this is not strictly required by the interface, in practice this is the
case: you can access any part of a <code>String</code> or
<code>CharBuffer</code>, or query its length, in any number of threads,
concurrently. <code>LargeText</code> must offer the same guarantees for it to
pretend being a <code>CharSequence</code> at all.</p>

<p>Also, accesses can be completely random. One thread can ask to read from
the beginning of the sequence, the other at the middle, the other at the end.
<code>LargeText</code> must be able to deal with that as well.</p>

<p>And of course, there is the not so trivial problem of {@link
java.lang.CharSequence#subSequence .subSequence()}.</p>

<h2>Memory usage should be controlled -- and controllable</h2>

<p>Dealing with large files of course has impacts on memory; if you use, say, a
    2 GiB file, you don't want the whole of it to eat valuable heap space.</p>

<p>The file must therefore be processed in chunks -- <em>without</em> altering
    the "streamlineness" of <code>CharSequence</code> which has no such notions.</p>

<h2>A decoding process is needed</h2>

<p>And this problem is pretty much unique to largetext. Its source is not a
character source but a <em>byte</em> source; what is more, said source can fail:
I/O errors, unmappable byte sequence etc.</p>

<p>Adding to that the fact that errors can happen at any moment. Fun!</p>

<p>And about that byte vs char problem...</p>

<h2>Character codings are often not fixed in length</h2>

<p>Starting with the now universally (?) used UTF-8 character coding: a single
Unicode character can be encoded using anywhere from one byte to four bytes.</p>

<p>Which means you can experience "short reads" (an incomplete sequence) when
decoding a given byte array within the source file. This also needs to be
dealt with.</p>

<p>And finally there is that:</p>

<h2><code>.toString()!</code></h2>

<p>Yes, the most trivial and normally innocuous operation on any Java object is
a death trap with this package! The <a
href="http://docs.oracle.com/javase/8/docs/api/java/lang/CharSequence.html#toString--"
target="_blank">javadoc</a> of <code>CharSequence</code> says:</p>

<blockquote>
    Returns a string containing the characters in this sequence in the same
    order as this sequence. <strong>The length of the string will be the length
    of this sequence.</strong>
</blockquote>

<p>This means a string as large as a file which is potentially <em>several
gigabytes in size</em>!</p>

<p>There should therefore be an option to break this contract... You certainly
don't want to be greeted with an {@link java.lang.OutOfMemoryError} just because
you inadvertently used a <code>LargeText</code> instance in a string
concatenation!</p>

<h2>And, uh, is that all?</h2>

<p>At least, that is the list of all the problems I could see. And largetext
has a solution for each of them!</p>

<h1>Using it</h1>

<p>This is a bare bones, standard use of this API:</p>

<pre>
    // Default factory: UTF-8 character encoding, 2 MiB window size
    final LargeTextFactory factory = LargeTextFactory.defaultFactory();
    final Path fileToSearch = Paths.get("path/to/large/textfile.txt");
    final Pattern pattern = Pattern.compile(...);

    try (
        final LargeText input = factory.fromPath(fileToSearch);
    ) {
        final Matcher m = pattern.matcher(input);
        if (m.find())
            System.out.println(fileToSearch + " has a match");
    }
</pre>

<p>The process is simple: create (and, if needed, configure) a {@link
com.github.fge.largetext.LargeTextFactory}, grab a {@link java.nio.file.Path}
to the file we want to use, create a {@link com.github.fge.largetext.LargeText}
instance using this factory and path, and use it.</p>

<p>You will note the use of a try-with-resources statement.
<code>LargeText</code> implements {@link java.io.Closeable} in addition to
<code>CharSequence</code> and in order to not leak any resources (starting with
the file descriptor), you must, at the very least, {@link
java.io.Closeable#close() .close()} it!</p>

<p>The comment on the first line hints that the factory is configurable. Here is
an example of it:</p>

<pre>
    // Create a factory with ISO-8859-15 encoding and a 12 MiB window size
    final LargeTextFactory factory = LargeTextFactory.newBuilder()
        .setCharsetByName("ISO-8859-15")
        .setWindowSize(12, SizeUnit.MiB)
        .build();
</pre>

<p>The {@link
com.github.fge.largetext.LargeTextFactory.Builder#setCharsetByName} method tells
which character coding should be used when decoding the file contents, and the
{@link com.github.fge.largetext.LargeTextFactory.Builder#setWindowSize} method
tells how large a chunk of bytes the decoding process should use from the file.
</p>

<p><strong>TODO: .toString() threshold!</strong></p>

<h1>Short overview of the internals...</h1>

<h2>The decoding process</h2>

<p>The decoding process is done with the {@link
com.github.fge.largetext.load.TextDecoder} class; one instance of this class is
created for each <code>LargeText</code> instance.</p>

<p>This class will create a background thread which is the real workhorse; it
will not only (attempt to) decode the whole contents of the file, but
it will also signal waiters (see below) to tell them that the number of
characters they wanted is now available.</p>

<h2>Caching character contents</h2>

<p>The decoding process, by itself, will not cache anything, it is just here to
ensure that at least this number of characters can be read from the file. The
role of actually providing these characters is delegated to a {@link
com.github.fge.largetext.load.TextCache}. The "unit" of a cached entry is a
{@link com.github.fge.largetext.load.TextRange}, which maps a byte window of the
input file to a character window.</p>

<p>This cache loads character "windows" on demand, and has an expiry policy of
30 seconds after last access; if no thread requires the same buffer within this
delay, it is discarded from the cache.</p>

<h2>Dealing with potentially waiting operations...</h2>

<p>All operations from <code>CharSequence</code> can potentially wait for the
required number of characters to be available; all of them for
<code>.length()</code>, or part of them for <code>.charAt()</code> and
<code>.subSequence()</code>.</p>

<p>Callers of <code>.length()</code> will all share a same {@link
java.util.concurrent.CountDownLatch} on which they will wait for the decoding
process to wake them up.</p>

<p>For the two other operations, a {@link
com.github.fge.largetext.load.CharWaiter} instance will be created; it will
be queued <em>if and only if</em> the requested number of characters is not
available at the time of the call. It will then be woken up when the number of
requested character are available, or "greeted" with an exception if it is not
(either because the requested index is out of range, or because the decoding
process failed).</p>

<h2>And there is more...</h2>

<p>All the nooks and crannies can not be explained in this single page; if you
want to know more, you are encouraged to read the javadoc of all packages and
classes!</p>

<p>Have fun!</p>

</body>
</html>